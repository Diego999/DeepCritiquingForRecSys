{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import PerceptronTagger\n",
    "from nltk.data import find\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemID = 'asin'\n",
    "userID = 'reviewerID'\n",
    "rating = 'overall'\n",
    "reviewText = 'reviewText'\n",
    "summary = 'summary'\n",
    "helpful = 'helpful'\n",
    "helpful_rating = 'helpful_rating'\n",
    "total_helpful = 'total_helpful'\n",
    "overall = 'overall'\n",
    "dataPath = 'data/ToysGames/'\n",
    "dataName = 'reviews_Toys_and_Games_5.json.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF(dataPath+dataName)[[userID, itemID, helpful, reviewText, overall, summary]]\n",
    "df[reviewText] = df[reviewText].astype('str')\n",
    "df[summary] = df[summary].astype('str')\n",
    "df[helpful] = df[helpful].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df[[\"reviewerID\", \"asin\", \"helpful\", \"reviewText\", \"overall\", \"summary\"]].to_csv(\"data/ToysGames/AmazonToyGamesRawData.csv\", sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(dataPath+dataName, sep='\\t', encoding='utf-8')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort by helpfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "df[helpful] = df[helpful].apply(lambda x: literal_eval(x))\n",
    "df[helpful_rating] = df[helpful].apply(lambda x: x[0])\n",
    "df[total_helpful] = df[helpful].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>helpful_rating</th>\n",
       "      <th>total_helpful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46315</th>\n",
       "      <td>A1OUQCTNVKPVR9</td>\n",
       "      <td>B0010VS078</td>\n",
       "      <td>[1589, 1637]</td>\n",
       "      <td>I loaned my iPod to my kid and he broke it.  T...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>It's a great portable music solution</td>\n",
       "      <td>1589</td>\n",
       "      <td>1637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103098</th>\n",
       "      <td>A4LD7XC56J3ZV</td>\n",
       "      <td>B004Z7H07K</td>\n",
       "      <td>[1431, 1502]</td>\n",
       "      <td>Hi! I am Erin T. and I run a website called th...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>My Son Won't Put it Down</td>\n",
       "      <td>1431</td>\n",
       "      <td>1502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131030</th>\n",
       "      <td>A1SC7Z2646QCP9</td>\n",
       "      <td>B0089RPUHO</td>\n",
       "      <td>[1413, 1449]</td>\n",
       "      <td>If you want a child-friendly tablet-style devi...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Hands down the best choice for a child-friendl...</td>\n",
       "      <td>1413</td>\n",
       "      <td>1449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80422</th>\n",
       "      <td>A3DZFEICHK5LF2</td>\n",
       "      <td>B003JQT4Y0</td>\n",
       "      <td>[1378, 1393]</td>\n",
       "      <td>Short version:The good: The pen is amazing, a ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Great product but a lot more parent involvement.</td>\n",
       "      <td>1378</td>\n",
       "      <td>1393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103019</th>\n",
       "      <td>A2DG63DN704LOI</td>\n",
       "      <td>B004Z7H07K</td>\n",
       "      <td>[1291, 1359]</td>\n",
       "      <td>I really want to like the LeapPad - my kids do...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Kids like it, but educational value is not as ...</td>\n",
       "      <td>1291</td>\n",
       "      <td>1359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            reviewerID        asin       helpful  \\\n",
       "46315   A1OUQCTNVKPVR9  B0010VS078  [1589, 1637]   \n",
       "103098   A4LD7XC56J3ZV  B004Z7H07K  [1431, 1502]   \n",
       "131030  A1SC7Z2646QCP9  B0089RPUHO  [1413, 1449]   \n",
       "80422   A3DZFEICHK5LF2  B003JQT4Y0  [1378, 1393]   \n",
       "103019  A2DG63DN704LOI  B004Z7H07K  [1291, 1359]   \n",
       "\n",
       "                                               reviewText  overall  \\\n",
       "46315   I loaned my iPod to my kid and he broke it.  T...      4.0   \n",
       "103098  Hi! I am Erin T. and I run a website called th...      5.0   \n",
       "131030  If you want a child-friendly tablet-style devi...      5.0   \n",
       "80422   Short version:The good: The pen is amazing, a ...      3.0   \n",
       "103019  I really want to like the LeapPad - my kids do...      3.0   \n",
       "\n",
       "                                                  summary  helpful_rating  \\\n",
       "46315                It's a great portable music solution            1589   \n",
       "103098                           My Son Won't Put it Down            1431   \n",
       "131030  Hands down the best choice for a child-friendl...            1413   \n",
       "80422    Great product but a lot more parent involvement.            1378   \n",
       "103019  Kids like it, but educational value is not as ...            1291   \n",
       "\n",
       "        total_helpful  \n",
       "46315            1637  \n",
       "103098           1502  \n",
       "131030           1449  \n",
       "80422            1393  \n",
       "103019           1359  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=[helpful_rating], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>helpful_rating</th>\n",
       "      <th>total_helpful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [reviewerID, asin, helpful, reviewText, overall, summary, helpful_rating, total_helpful]\n",
       "Index: []"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[reviewText].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_groupby_asin = df.groupby(itemID).agg({reviewText:','.join, summary:','.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_groupby_asin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Review\n",
    "df[\"review_word_tokenized\"] = df[reviewText].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# Remove Stopwords\n",
    "# Get english stopwords\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "df[\"review_word_tokenized\"] = df[\"review_word_tokenized\"].apply(lambda text: \n",
    "                                                                [w for w in text if w not in en_stopwords])\n",
    "\n",
    "# Remove Punctuation\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "df[\"review_word_tokenized\"] = df[\"review_word_tokenized\"].apply(lambda text: \n",
    "                                                                [w.translate(table) for w in text])\n",
    "\n",
    "# Remove tokens that are not alphabetic\n",
    "df[\"review_word_tokenized\"] = df[\"review_word_tokenized\"].apply(lambda text: \n",
    "                                                                [w for w in text if w.isalpha()])\n",
    "\n",
    "# Lowercase\n",
    "df[\"review_word_tokenized\"] = df[\"review_word_tokenized\"].apply(lambda text: \n",
    "                                                                [w.lower() for w in text])\n",
    "\n",
    "# Lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df[\"review_word_tokenized\"] = df[\"review_word_tokenized\"].apply(lambda text: \n",
    "                                                                [lemmatizer.lemmatize(w) for w in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "text = df[reviewText].tolist()\n",
    "tokenizer.fit_on_texts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_index = pd.DataFrame(list(tokenizer.word_index.items()), columns=['word','index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_word_index.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = df[reviewText].apply(lambda x: nltk.word_tokenize(x)+[' ']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter ADJ/NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to filter for ADJ/NN bigrams\n",
    "def rightTypes(ngram):\n",
    "    if '-pron-' in ngram or 't' in ngram:\n",
    "        return False\n",
    "    for word in ngram:\n",
    "        if word.isspace():\n",
    "            return False\n",
    "    acceptable_types = ('JJ', 'JJR', 'JJS')\n",
    "    ins = ('IN','TO')\n",
    "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    \n",
    "    if len(tags) == 2:\n",
    "        if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif len(tags) == 3:\n",
    "        if tags[0][1] in acceptable_types and tags[1][1] in ins and tags[2][1] in second_type:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        if tags[0][1] in acceptable_types and tags[1][1] in ins and tags[2][1] in acceptable_types and tags[3][1] in second_type:\n",
    "            return True\n",
    "        else:\n",
    "            return False  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "tokens = itertools.chain.from_iterable(token_list)\n",
    "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
    "\n",
    "bigram_freq = bigramFinder.ngram_fd.items()\n",
    "bigramFreqTable = pd.DataFrame(list(bigram_freq), \n",
    "                               columns=['ngram','freq']).sort_values(by='freq', ascending=False)\n",
    "# bigramFreqTable = bigramFreqTable[bigramFreqTable.ngram.map(lambda x: rightTypes(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bigramFreqTable.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigrams = nltk.collocations.TrigramAssocMeasures()\n",
    "# tokens = itertools.chain.from_iterable(token_list)\n",
    "# trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(tokens)\n",
    "# trigram_freq = trigramFinder.ngram_fd.items()\n",
    "\n",
    "# trigramFreqTable = pd.DataFrame(list(trigram_freq), \n",
    "#                                 columns=['ngram','freq']).sort_values(by='freq', ascending=False)\n",
    "# trigramFreqTable = trigramFreqTable[trigramFreqTable.ngram.map(lambda x: rightTypes(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trigramFreqTable.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = nltk.collocations.TrigramCollocationFinder.from_words([\"what\",\"the\",\"hell\"])\n",
    "finder.ngram_fd.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bigramFinder.apply_freq_filter(100)\n",
    "bigramPMITable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.pmi)), \n",
    "                              columns=['bigram','PMI']).sort_values(by='PMI', ascending=False)\n",
    "bigramPMITable = bigramPMITable[bigramPMITable.bigram.map(lambda x: rightTypes(x))]\n",
    "bigramPMITable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bigramPMITable = bigramPMITable[bigramPMITable.bigram.map(lambda x: rightTypes(x))]\n",
    "# bigramPMITable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bigram_freq_pmi = pd.merge(bigramFreqTable, bigramPMITable, how='right', left_on='ngram', right_on='bigram').sort_values(\"PMI\", ascending=False)\n",
    "bigram_freq_pmi.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_freq_pmi.head(50)['freq'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_freq_pmi.head(50)['PMI'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_keyphrases = pd.concat([bigramFreqTable[['bigram']].head(50), bigramPMITable[['bigram']].head(50)])\n",
    "df_keyphrases = bigram_freq_pmi[['bigram']].head(50)\n",
    "df_keyphrases['Phrases'] = df_keyphrases['bigram'].apply(lambda x: ' '.join(x))\n",
    "df_keyphrases = df_keyphrases[['Phrases']].reset_index(drop=True)\n",
    "keyphrases = df_keyphrases['Phrases'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"keyphrases_indices\"] = df[reviewText].apply(lambda x: [keyphrases.index(key) for key in keyphrases if key in x])\n",
    "df['keyphrases_indices_length'] = df['keyphrases_indices'].str.len()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df['keyphrases_indices_length'].sum())\n",
    "print(df['keyphrases_indices_length'].sum() / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column='keyphrases_indices_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokenizer\"] = df[reviewText].apply(lambda x: tokenizer.fit_on_texts(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df[reviewText].apply(lambda x: [1. if key in x else 0 for key in keyphrases])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['keyVector'] = df[reviewText].apply(lambda x: [1. if key in x else 0 for key in keyphrases])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df['keyIndices'] = df['keyVector'].apply(lambda vector: [i for i, x in enumerate(vector) if x == 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['keyIndices'].str.len().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['UserIndex'] = df[userID].astype('category').cat.rename_categories(range(0, df[userID].nunique()))\n",
    "df['ItemIndex'] = df[itemID].astype('category').cat.rename_categories(range(0, df[itemID].nunique()))\n",
    "df['Binary'] = (df[rating] > 3)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_name = df[['UserIndex',userID]]\n",
    "df_item_name = df[['ItemIndex',itemID]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop([itemID, userID, reviewText], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keyphrases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item_name.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of User: {0}\".format(df_user_name['UserIndex'].nunique()))\n",
    "print(\"Number of Item: {0}\".format(df_item_name['ItemIndex'].nunique()))\n",
    "print(\"Number of Positive Review: {0}\".format(sum(df['Binary'].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vector'] = tokenizer.texts_to_sequences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_lengths = df.vector.apply(lambda x: len(x)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.hist(review_lengths, normed=True, bins=30)\n",
    "plt.ylabel('Probability');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(dataPath+'Data.csv')\n",
    "df_word_index.to_csv(dataPath+'WordIndex.csv')\n",
    "df_keyphrases.to_csv(dataPath+'KeyPhrases.csv')\n",
    "df_user_name.to_csv(dataPath+'UserIndex.csv')\n",
    "df_item_name.to_csv(dataPath+'ItemIndex.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
